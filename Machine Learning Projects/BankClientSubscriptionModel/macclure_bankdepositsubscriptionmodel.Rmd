---
title: "Client Term Bank Deposit Subscription Model"
author: "Doug MacClure"
date: "5/13/2019"
output: rmarkdown::github_document
fig_width: 6

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

In this report, we analyze the Bank Marketing Data Set (found here: <https://archive.ics.uci.edu/ml/datasets/Bank+Marketing>) and construct various classification models using 70% of the bank marketing dataset.  Various classification algorithms are discussed, and our approach using a random forest classification model is discussed.  

This project is done for the Data Science Capstone assignment via HarvardX.  The goal of this model is to create a classification model which predicts the binary outcome of whether or not a client will purchase a bank deposit subscription given demographic, transactional and industry data.  Here, a positive result in the model (where $y = 0$) corresponds to a client **not** purchasing a subscription and a negative result (where $y = 1$) corresponds to a client purchasing a subscription.  Hence, we wish to maximize both accuracy and specificity.  

By specificity, we mean the proportion of clients who actually subscribed and clients the model has predicted to subscribe.  In other words, for 
\[TN = \text{ true negative, i.e., model correctly predicts client subscribed} \]

\[FP = \text{ false positive, i.e., model incorrectly predicts client didn't subscribe, when client actually subscribed} \]
then we wish to maximize
\[sensitivity = \frac{TN}{TN+FP}.\]
Clearly, since, $TN,FP \geq 0$, then $sensitivity \in [0,1]$.

In this report, we use a random forest classification model using R 3.6 and the **randomForest** library.  Random forests utilize the bagging method together with decision trees.  In other words, the training data is subsetted in multiple ways randomly, where on each subset a decision tree is trained.  The results of these trees are combined and a value is determined through voting.  Recall, tree-based models partition the data according to cutoff values for predictors determined by the Gini index and entropy, which weigh the best possible proportions of these predictors.  

Tree-based models are known to capture non-linearity in the data.  Since we are attempting to model human behavior not at a macroscopic level, it is perhaps not surpising that the data appears to have non-linear trends.  

To begin, perform the following steps: install (if necessary) and load the required packages to run the R code: caret, tidyverse.  Next, download the required data, and wrangle/coerce data for analysis.  Finally, split the prepared data into training and validation datasets. 

```{r load libraries, include=FALSE}
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(gdata)) install.packages("gdata", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

if(!require(e1071)) install.packages("kknn", repos = "http://cran.us.r-project.org")
if(!require(naive_bayes)) install.packages("naive_bayes", repos = "http://cran.us.r-project.org")
if(!require(randomForest)) install.packages("randomForest", repos = "http://cran.us.r-project.org")
if(!require(factoextra)) install.packages("factoextra", repos = "http://cran.us.r-project.org")
if(!require(gplots)) install.packages("gplots", repos = "http://cran.us.r-project.org")
if(!require(Hmisc)) install.packages("Hmisc", repos = "http://cran.us.r-project.org")


```

  

```{r getdata, include=FALSE}
dl <- tempfile()
download.file("https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank-additional.zip", dl)
#unzfile <- unz(dl, "bank-additional/bank-additional.csv")
#data <- read.csv(unzfile)
data <- read.table(text = gsub(";", "\t", readLines(unzip(dl, "bank-additional/bank-additional-full.csv"))),
                      col.names = c("age", "job", "marital", "education",
                                    "default", "housing","loan", "contact", "month",
                                    "day_of_week", "duration", "campaign", "pdays", "previous","poutcome", "emp.var.rate", "cons.price.idx",
                                    "cons.conf.idx", "euribor3m", "nr.employed",   "y"))
```

 

```{r make train/test sets, include=FALSE}

#remove first row
data1 <- data%>%filter(age!='age')
#etl data for ml
all_num <- as.data.frame(data1) %>% mutate(age = as.numeric(levels(age))[age],
                               job = as.numeric(job),
                               marital = as.numeric(marital),
                               education = as.numeric(education),
                               default = as.numeric(default),
                               housing = as.numeric(housing),
                               loan = as.numeric(loan),
                               contact = as.numeric(contact),
                               month = as.numeric(month),
                               day_of_week = as.numeric(day_of_week),
                               duration = as.numeric(levels(duration))[duration],
                               campaign = as.numeric(levels(campaign))[campaign],
                               pdays = as.numeric(levels(pdays))[pdays],
                               previous = as.numeric(levels(previous))[previous],
                               poutcome = as.numeric(poutcome),
                               emp.var.rate = as.numeric(levels(emp.var.rate))[emp.var.rate],
                               cons.price.idx = as.numeric(levels(cons.price.idx))[cons.price.idx],
                               cons.conf.idx = as.numeric(levels(cons.conf.idx))[cons.conf.idx],
                               euribor3m = as.numeric(levels(euribor3m))[euribor3m],
                               nr.employed = as.numeric(levels(nr.employed))[nr.employed],
                               y = as.numeric(y)) 
all_num <- all_num[,-11]
all_num_y <- all_num$y
all_num$y <- NULL
all_num_y <- ifelse(all_num_y == 3, 1, 0)
set.seed(1)

```

We begin our analysis of the data by noting which potential predictive variables we have to work with, and how they're correlated.  First, observe that we should not include all variables from the zip file in the model.  In particular, duration is included in the zipped .csv file.  One will soon see that it is ALWAYS important to read ALL information given on the data before performing an analysis.  The original author of the data wrote the following:


**DEFINITION: duration-** *last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.*

For more information on other variables, see: <https://archive.ics.uci.edu/ml/datasets/Bank+Marketing>

```{r variables, echo=FALSE}

head1 <- all_num %>%select(age, marital, education, default,housing,
                           loan,month, day_of_week, campaign)
head2 <- all_num %>% select(pdays, previous, poutcome,emp.var.rate, cons.price.idx,
                            cons.conf.idx,euribor3m, nr.employed)
head(head1) %>% knitr::kable()
head(head2) %>% knitr::kable()


corr <- rcorr(as.matrix(all_num))
y<-corr$r
my_image <- function(x, zlim = range(x), ...){
  colors = rev(RColorBrewer::brewer.pal(9, "RdBu"))
  cols <- 1:ncol(x)
  rows <- 1:nrow(x)
  image(cols, rows, t(x[rev(rows),,drop=FALSE]), xaxt = "n", yaxt = "n",
        xlab="", ylab="",  col = colors, zlim = zlim, ...)
  abline(h=rows + 0.5, v = cols + 0.5)
  #axis(side = 1, cols, colnames(x), las = 2)
  angleAxis(side = 1, labels = colnames(x), at = 1:length(colnames(x)), srt = 45, xpd = TRUE)
}

my_image(corr$r)

```

We observe that three variables in particular are highly correlated to each other, as can b.  Namely, nr.employed, which indicates the number of people employed, euribor3m, which indicates the euribor 3 month rate (where euribor is based on the averaged interest rates at which Eurozone banks offer to lend unsecured funds to other banks in the euro wholesale money market), and finally the emp.var.rate, which is the employment variation rate.  Clearly, it makes sense that these three are related, since a bank will have more employees during a good economy.  However, we will not remove two of these variables, since they capture different information.  If necessary, the author suggests to consider regularization.  

Of greatest importance in this model is that we consider the prevalence of the target variable in the model.  Observe the distribution of the target variable.  Recall that the target y, describes whether or not a client has subscribed for a bank deposit.  

```{r distribution of values for target variable, echo=FALSE}
hist(all_num_y)
```

We observe that 89% of the clients do not pay for a subscription.  Hence, we should be careful in how we interpret such metrics as model accuracy.  It is important to consider, since we can have a model with 100% specificity simply by labelling all outcomes as subscriptions (i.e., y = 1).  In this case, the accuracy would be around 11%.  We wish to strike a good balance between the two metrics. 

Next, we wish to discern the dimensionality of the data.  The author was surprised to find out that the data is essentially 4 dimensional, since the first 4 principal components explain over 99.9% of the variance in the data.

```{r user quantity rating avg histogram, echo=FALSE}

#ANALYZE FIRST FEW PRINCIPAL COMPONENTS
#y <- sweep(all_num,1,rowMeans(all_num))
#all_num_y <- all_num$y
#all_num$y <- NULL

pca <- prcomp(as.matrix(all_num))
#plot(pca$sdev)

var <- get_pca_var(pca)
fviz_eig(pca, addlabels = TRUE, ylim = c(0, 90))

```

To see which variables contributed the most to the variance in the data, observe their importance towards the first four principal components.   

```{r perc. var. explained, echo=FALSE }
my_image1 <- function(x, zlim = range(x), ...){
  colors = rev(RColorBrewer::brewer.pal(9, "RdBu"))
  cols <- 1:ncol(x)
  rows <- 1:nrow(x)
  image(cols, rows, t(x[rev(rows),,drop=FALSE]), xaxt = "n", yaxt = "n",
        xlab="", ylab="",  col = colors, zlim = zlim, ...)
  abline(h=rows + 0.5, v = cols + 0.5)
  #axis(side = 1, cols, colnames(x), las = 2)
  #axis(side=2, rows, rownames(x), las = 2)
  angleAxis(side = 1, labels = colnames(x), at = 1:length(colnames(x)), srt = 45, xpd = TRUE)
  angleAxis(side = 2, labels = rownames(x), at = 1:length(rownames(x)), srt = 45, xpd = TRUE)
  
}

my_image1(var$contrib[,1:4])
```

Here, we see that housing, age, nr.employed and marital status contribute the most to the first four principal components.  Note that highly correlated data can change the direction of the first principal component, since it by definition points in the direction of greatest variance.  Hence, we will remove the variables nr.employed and emp.var.rate from the model and see if the PCA is different (i.e., seems to favor the highly correlated variables more).  We indeed see a change.


```{r day_of_week hist, echo=FALSE}
all_num_1 <- all_num[-19]
all_num_1 <- all_num_1[-18]


pca1 <- prcomp(as.matrix(all_num_1))
#plot(pca1$sdev)
fourPCs_1 <- data.frame(x1 = pca1$x[,1], x2 = pca1$x[,2], 
                      x3 = pca1$x[,3], x4 = pca1$x[,4])

withpcs <- cbind(fourPCs_1, all_num_1)
corr <- rcorr(as.matrix(withpcs))
rs<-corr$r
my_image(rs)

var <- get_pca_var(pca1)
my_image1(var$contrib[,1:4])
fviz_eig(pca1, addlabels = TRUE, ylim = c(0, 100))

```

Note now that the variables which explain the greatest variance now have changed.  In fact, it appears that the variable housing describes almost all of the variance in the model.  To see why, observe its distribution and the even spread of the data.

```{r housing, echo=FALSE}
hist(all_num_1$housing)
```
However, the author has noticed a marked decline in predictive power for all models when removing highly correlated variables.  Thus, the model should not be viewed as fundamentally one dimensional.  Now, if we consider how model variables are correlated with the first four components prior to removing the correlated variables, some other patterns emerge.  We will soon see that several of the predictors correlated with the first four principal components also are considered to be predictors of high importance for our final model.  

```{r perc. var. explained correlation, echo=FALSE}
pca <- prcomp(as.matrix(all_num))
fourPCs <- data.frame(x1 = pca$x[,1], x2 = pca$x[,2], 
                      x3 = pca$x[,3], x4 = pca$x[,4])


withpcs <- cbind(fourPCs, all_num)
corr <- rcorr(as.matrix(withpcs))
rs<-corr$r
my_image(rs)
```

Does this imply that these correlated variables will have the greatest importance in a model?  We will see.  

## Analysis

We begin our analysis by first observing that we will partition the data as follows: 70% train, 30% test.  Note, that it is extremely important that we take prevalence into account here, since there is a roughly 89%-11% split among positive-negative results.  Hence, it is extremely important to use the createDataPartition() function from the caret package, since prevalence is preserved.  

Throughout the rest of this analysis, we will compare the accuracies and specificities in using all of the data (sans the duration variable, since that variable cannot be known prior to model implementation) versus only using the model with the two aforementioned correlated variables removed.  To begin, we consider the k-nearest neighbors algorithm using all model variables.  

**WITH CORRELATED VARIABLES - REDUCED MODEL**

```{r knn check 1, echo=FALSE}
set.seed(1)
test_ind <- createDataPartition(y=all_num_y, times = 1, p = 0.7, list = FALSE)

train_x <- as.data.frame(all_num[test_ind,])
train_y <- as.factor(all_num_y[test_ind])

test_x <- as.data.frame(all_num[-test_ind,])
test_y <- as.factor(all_num_y[-test_ind])

knnAcc <- function(k){
  knn_fit <- knn3(train_x, as.factor(train_y), k=k)
  y_hat <- predict(knn_fit, test_x, type="class")
  confusionMatrix(data = y_hat, reference = as.factor(test_y))$overall["Accuracy"]
 # confusionMatrix(data = y_hat, reference = as.factor(test_y))$byClass["Specificity"]
}

knnSpec <- function(k){
  knn_fit <- knn3(train_x, as.factor(train_y), k=k)
  y_hat <- predict(knn_fit, test_x, type="class")
  confusionMatrix(data = y_hat, reference = as.factor(test_y))$byClass["Specificity"]
}




ks1 <- seq(1,15,2)
accs1 <- sapply(ks1,knnAcc)
specs1 <- sapply(ks1,knnSpec)
knn_1 <- data.frame(K = ks1, Accuracy = accs1, Specificity = specs1)

ks2 <- seq(20,50,5)
accs2 <- sapply(ks2, knnAcc)
specs2 <- sapply(ks2,knnSpec)
knn_2 <- data.frame(K = ks2, Accuracy = accs2, Specificity = specs2)

#knn_results <- cbind(knn_1, knn_2)

knn_1 %>% knitr::kable()
knn_2 %>% knitr::kable()
```

**WITHOUT CORRELATED VARIABLES - FULL MODEL**

```{r knn check 2, echo=FALSE}
set.seed(1)
test_ind <- createDataPartition(y=all_num_y, times = 1, p = 0.7, list = FALSE)

train_x1 <- as.data.frame(all_num_1[test_ind,])
train_y <- as.factor(all_num_y[test_ind])

test_x1 <- as.data.frame(all_num_1[-test_ind,])
test_y <- as.factor(all_num_y[-test_ind])

knnAcc1 <- function(k){
  knn_fit <- knn3(train_x1, as.factor(train_y), k=k)
  y_hat <- predict(knn_fit, test_x1, type="class")
  confusionMatrix(data = y_hat, reference = as.factor(test_y))$overall["Accuracy"]
 # confusionMatrix(data = y_hat, reference = as.factor(test_y))$byClass["Specificity"]
}

knnSpec1 <- function(k){
  knn_fit <- knn3(train_x1, as.factor(train_y), k=k)
  y_hat <- predict(knn_fit, test_x1, type="class")
  confusionMatrix(data = y_hat, reference = as.factor(test_y))$byClass["Specificity"]
}


ks3 <- seq(1,15,2)
accs3 <- sapply(ks3,knnAcc1)
specs3 <- sapply(ks3,knnSpec1)
knn_3 <- data.frame(K = ks3, Accuracy = accs3, Specificity = specs3)

ks4 <- seq(20,50,5)
accs4 <- sapply(ks4, knnAcc1)
specs4 <- sapply(ks4,knnSpec1)
knn_4 <- data.frame(K = ks4, Accuracy = accs4, Specificity = specs4)

#knn_results <- cbind(knn_3, knn_4)

knn_3 %>% knitr::kable()
knn_4 %>% knitr::kable()
```


Clearly, we see a small improvement in specificity when using the reduced model.  However, are we able to improve this result?  We will see that indeed we can.

```{r naive bayes full, include=FALSE}
set.seed(1)  
fit <- naiveBayes(train_x,as.factor(train_y))
y_hat <- predict(fit, test_x)

conf <- confusionMatrix(data = y_hat, reference = as.factor(test_y)) 


```

```{r naive bayes reduced, include=FALSE}
set.seed(1)  
fit <- naiveBayes(train_x,as.factor(train_y))
y_hat1 <- predict(fit, test_x1)

conf1 <- confusionMatrix(data = y_hat1, reference = as.factor(test_y)) 

```

```{r naive bayes output, echo=FALSE}
model <- c("Full", "Reduced")
specbayes <- c(conf$byClass["Specificity"], conf1$byClass["Specificity"]) 
accbayes <- c(conf$overall["Accuracy"], conf1$overall["Accuracy"])
bayesresults <- data.frame(model = model, accuracy = accbayes, specificity = specbayes)
bayesresults %>% knitr::kable()

```

Perhaps surprisingly, we see a marked decline in specificity from 47% to 37% when we use the reduced model instead of the full model.  In fact, the author has tried many different models using default parameters, where none were able to capture a specificity beyond 30%.  Hence, as of this moment, we have a potential candidate model: using the Naive Bayes method with all the data sans duration.  


However, the randomForest algorithm has given the best predictive power on the training set.  The author has also noticed a decline in Specificity when using the Reduced Model.  Thus, we use the Full Model moving forward.  

## Results

```{r randomForest, echo=FALSE}
library(randomForest)
set.seed(1)
rffit <- randomForest(test_x, test_y, importance = TRUE, mtry = 5)
#rffit
predTest <- predict(rffit, test_x, type = "class")

confusionMatrix(data = predTest, reference = as.factor(test_y))
```

We note the comparitively remarkable results of **82% specificity**, and **98% accuracy** in using the **randomForest** model with five variables tried at each split.  Next, consider the variable importance for this model.

```{r randomForest variable importance, echo=FALSE}
importance(rffit) %>% knitr::kable()
```

The top 5 most important variables for this model are **euribor3m, month, age, day_of_week** and **pdays**.  Note how some of these variables are correlated with some of the first four principal components.  Recall that a predictor is deemed important if permuting its values increases the model error.  Hence, a high mean decrease accuracy indicates a predictor which has a large impact on the model.  It is also important to note that despite the fact that the PCA seems to suggest that the predictor dataset is essentially four-dimensional from a variance perspective, yet there are no predictors whose mean decrease accuracy vanishes.  Hence, one should be careful when reducing the dimension of the dataset using PCA.   

Finally, note that we can reduce the complexity of our final model by reducing the number of trees from 500 to 100, as indicated in the following error plot (Note, that the error value for $1$ converges to $0.7$ .

```{r final model, echo=FALSE}
plot(rffit, ylim = c(0.0,0.8))
legend("right", colnames(rffit$err.rate), col=1:3, cex=0.7, fill=1:3)
```

## Conclusion

We have arrived at an acceptable model: using the **randomForest()** function with *mtry = 5*.  We achieve an **82% specificity** using this model together with a **98% accuracy** when evaluating the model on the test set.  

Hence, we advise the bank to use the model to target their marketing efforts towards clients who the model deem to be likely to purchase a subscription.  In using this model, it is likely that the bank can expect to cut mailing/advertising costs by 89% (due to prevalence) while still retaining 82% of the total revenue they would have obtained if they targeted the entire population.  

To further improve this model, the author suggests to further change hyperparameters in the randomForest() algorithm.  Another way in which this model could be improved is if we also considered neural networks and/or using an ensemble model approach.  